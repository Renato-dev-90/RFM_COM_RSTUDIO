install.packages('RMySQL')
install.packages("dbplyr")
library(RMySQL)
library(ggplot2)
library(dbplyr)
con = dbConnect(MySQL(), user = "root", password = "renatoeju15122018", dbname = "TITANICDB", host = "localhost")
qry <- "select * from titanic"
dbGetQuery(con, qry)
dados <- data.frame(dbGetQuery(con, qry))
dados
head(dados)
View(dados)
# Reshape 2
install.packages("reshape2")
library(reshape2)
# Configurando o diretório de trabalho
# Coloque entre aspas o diretório de trabalho que você está usando no seu computador
# Não use diretórios com espaço no nome
setwd(
"C:/Users/ACER/Desktop/DATA_SCIENTIST/01-BIG_DATA_ANALYTISC_COM_R_E_MICROSOFT_AZURE_MACHINE_LERNING/08-I
NTRODUCAO_A_ANALISE_ESTATISTICA_PARTE_1")
getwd()
# Formatando os dados de uma página web
library(rvest)
# Formatando os dados de uma página web
library(rvest)
# Formatando os dados de uma página web
install.packages("rvest")
library(rvest)
library(stringr)
library(tidyr)
# Exercício 1 - Faça a leitura da url abaixo e grave no objeto pagina
# http://forecast.weather.gov/MapClick.php?lat=42.31674913306716&lon=-71.42487878862437&site=all&smap=1#.VRsEpZPF84I
pagina <- read_html("http://forecast.weather.gov/MapClick.php?lat=42.31674913306716&lon=-71.42487878862437&site=all&smap=1#.VRsEpZPF84I")
pagina
View(pagina)
View(pagina)
pagina
as.data.frame(pagina)
pagina
# Exercício 2 - Da página coletada no item anterior, extraia o texto que contenha as tags:
# "#detailed-forecast-body b  e .forecast-text"
previsao <- html_nodes(pagina, "#detailed-forecast-body b , .forecast-text")
previsao
# Exercício 3 - Transforme o item anterior em texto
texto <- html_text(previsao)
paste(texto, collapse = " ")
?collapse
# Exercício 4 - Extraímos a página web abaixo para você. Agora faça a coleta da tag "table"
url <- 'http://espn.go.com/nfl/superbowl/history/winners'
pagina <- read_html(url)
tabela <- html_nodes(pagina, 'table')
class(tabela)
tabela
View(tabela)
# Exercício 5 - Converta o item anterior em um dataframe
tab <- html_table(tabela)[[1]]
class(tab)
head(tab)
# Exercício 6 - Remova as duas primeiras linhas e adicione nomes as colunas
tab <- tab[-(1:2), ]
head(tab)
names(tab) <- c("number", "date", "site", "result")
head(tab)
# Exercício 7 - Converta de algarismos romanos para números inteiros
tab$number <- 1:52
# Exercício 7 - Converta de algarismos romanos para números inteiros
tab$number <- 1:52
# Exercício 7 - Converta de algarismos romanos para números inteiros
tab$number <- 1:56
tab$date <- as.Date(tab$date, "%B. %d, %Y")
head(tab)
# Exercício 8 - Divida as colunas em 4 colunas
tab <- separate(tab, result, c('winner', 'loser'), sep = ', ', remove = TRUE)
head(tab)
# Exercício 9 - Inclua mais 2 colunas com o score dos vencedores e perdedores
# Dica: Você deve fazer mais uma divisão nas colunas
pattern <- " \\d+$"
tab$winnerScore <- as.numeric(str_extract(tab$winner, pattern))
tab$loserScore <- as.numeric(str_extract(tab$loser, pattern))
tab$winner <- gsub(pattern, "", tab$winner)
tab$loser <- gsub(pattern, "", tab$loser)
head(tab)
# Exercício 10 - Grave o resultado em um arquivo csv
write.csv(tab, 'superbowl.csv', row.names = F)
dir()
# Obs: Caso tenha problemas com a acentuação, consulte este link:
# https://support.rstudio.com/hc/en-us/articles/200532197-Character-Encoding
# Configurando o diretório de trabalho
# Coloque entre aspas o diretório de trabalho que você está usando no seu computador
# Não use diretórios com espaço no nome
setwd(
"C:/Users/ACER/Desktop/DATA_SCIENTIST/
01-BIG_DATA_ANALYTISC_COM_R_E_MICROSOFT_AZURE_MACHINE_LERNING/09-INTRODUCAO_A_ANALISE_ESTATISTICA_PARTE_2"
)
getwd()
list.files()
search()
library(dplyr)
library(arules)
library(arulesViz)
library(htmlwidgets)
library(writexl)
library(readxl)
dados <- read_excel("ABRIL_2022.xlsx")
View(dados)
setwd("C:/Users/ACER/Desktop/DSA/RFM")
getwd()
# Carregando os pacotes
library(dplyr)
library(arules)
library(arulesViz)
library(htmlwidgets)
library(writexl)
library(readxl)
# Carrengando e explorando o dataset
dados <- read_excel("SKUs_ABRIL_2022.xlsx")
dim(dados)
summary(dados)
str(dados)
View(dados)
# Verificando se temos valores ausentes no primeiro e segundo item de compra
sum(is.na(dados$`1`))
sum(is.na(dados$`2`))
# Verificando se temos valores ausentes representados por espaço em branco (usando expressão regular)
grepl("^\\s*$", dados$`2`)
# Número de itens distintos
n_distinct(dados)
# Número de itens distintos no segundo item de compra
n_distinct(dados$`2`)
# Preparando o dataset e convertendo as variáveis para o tipo fator
# (variáveis que usaremos daqui em diante)
dados_02 <- dados
dados_02$`1` <- as.factor(dados_02$`1`)
dados_02$`2` <- as.factor(dados_02$`2`)
# Utilizarei apenas a associação de  2 produtos
dados_03 <- select(dados_02, `1`, `2`)
dados_03
# Verificando as regras:
regras_abril <- apriori(dados_03, parameter = list(supp = 0.027, conf = 0.9))
# Filtrando as regras redundantes
regras_abril_clean <- regras_abril[!is.redundant(regras_abril)]
# Regras
regras_abril_clean
# Inspeção das regras
inspect(regras_abril_clean)
# Convertendo para um dataframe
abril_df <- as(regras_abril_clean, "data.frame")
# Visualizando o dataframe, ordenado pelo maior número de associções
View(arrange(abril_df, desc(count)))
# Plotando
plot(regras_abril_clean, measure = "support", shading = "confidence", method = "graph", engine = "html")
# Salvando o arquivo no formato xlsx
write_xlsx(abril_df, "ASSOCIACOES_ABRIL_2022.xlsx")
barplot(regras_abril_clean)
barplot(c(regras_abril_clean))
abrilll <- c(regras_abril_clean)
barplot(abrilll)
abrilll <- matrix(regras_abril_clean)
